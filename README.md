# `MagnusFormer`: gerando partidas de Xadrez com Transformers.
# `MagnusFormer`: generating Chess games with Transformers.

## Apresenta√ß√£o

O presente projeto foi originado no contexto das atividades da disciplina de p√≥s-gradua√ß√£o *IA376L - Deep Learning aplicado a S√≠ntese de Sinais*, 
oferecida no primeiro semestre de 2022, na Unicamp, sob supervis√£o da Profa. Dra. Paula Dornhofer Paro Costa, do Departamento de Engenharia de Computa√ß√£o e Automa√ß√£o (DCA) da Faculdade de Engenharia El√©trica e de Computa√ß√£o (FEEC).

|Nome  | RA | Especializa√ß√£o|
|--|--|--|
| Ronnypetson Souza da Silva  | 211848  | Ci√™ncia da Computa√ß√£o|


## Descri√ß√£o Resumida do Projeto

### Tema do projeto, contexto e motiva√ß√£o

O tema do projeto √© a gera√ß√£o de partidas de Xadrez, usando a nota√ß√£o PGN (*Portable Game Notation*), que sejam semelhantes √† partidas entre profissionais do jogo. Para tanto, deseja-se usar modelos de linguagem autorregressivos baseados em Transformer, tais como BERT. Partidas de Xadrez sint√©ticas semelhante √†s que humanos jogam podem ser usadas na literatura e no cinema (ex: *O Gambito da Rainha*, *Os Simpsons*, *Death Note*, *Harry Potter*), com gera√ß√£o condicionada e flexibilidade. Por ser compat√≠vel com modelos de linguagem e por possuir muitas partidas p√∫blicas que a usam, a nota√ß√£o PGN √© uma escolha adequada para a tarefa em quest√£o.

Os modelos de linguagem autorregressivos baseados em Transformer t√™m mudado o estado-da-arte em v√°rias tarefas de linguagem natural, tais como tradu√ß√£o, reponder √† perguntas, racioc√≠nio de bom senso e interpreta√ß√£o de texto. Tamb√©m t√™m sido usados na gera√ß√£o de c√≥digo em linguagens de programa√ß√£o, imagens vetorizadas tipo SVG e arquivos no formato MIDI. A princ√≠pio, uma base de textos em  qualquer linguagem, natual ou n√£o, com um vocabul√°rio de tamanho razo√°vel (menor ou compar√°vel com o vocabul√°rio de uma linguagem natural como o Ingl√™s), pode ser modelada com esse tipo de t√©cnica. Isso inclui os textos nos arquivos PGN de Xadrez, que possuem um vocabul√°rio - cerca de 14700 *tokens* - bem menor do que o do Ingl√™s.

### Objetivo

O objetivo principal do projeto √© a cria√ß√£o de um modelo generativo que seja capaz de produzir sequ√™ncias de lances de Xadrez no formato PGN, de modo que gere partidas de Xadrez semelhantes √† partidas jogadas por seres humanos no n√≠vel profissional (partidas jogadas em torneios). O modelo deve ser capaz de gerar partidas do in√≠cio como tamb√©m de completar partidas j√° come√ßadas.

### Sa√≠da do modelo

A sa√≠da do modelo ser√° uma sequ√™ncia de anota√ß√µes de lances de Xadrez numa vers√£o simplificada do formato PGN. Nesse formato, cada lance especifica uma pe√ßa no tabuleiro e a nova casa que deve ocupar. As pe√ßas, exceto pe√µes, s√£o representados por letras da seguinte forma:

* Pe√£o: ‚ôô‚ôü
* Cavalo: N (kNight) ‚ôò‚ôû
* Bispo: B (Bishop) ‚ôó‚ôù
* Torre: R (Rook) ‚ôñ‚ôú
* Dama: Q (Queen) ‚ôï‚ôõ
* Rei: K (King) ‚ôî‚ôö

As linhas e colunas do tabuleiro s√£o representadas pelos n√∫meros de ```1``` √† ```8``` e pelas letras de ```a``` √† ```h```, como mostra a figura abaixo. Dessa maneira, quando queremos dizer "mover o cavalo para a casa f3", escrevemos ```Nf3``` na nota√ß√£o PGN. Quando a pe√ßa a ser movida √© amb√≠gua, pode-se especificar a coluna e/ou a linha da casa que a pe√ßa ocupa caso seja necess√°rio, como por exemplo ```Ngf3```, ```N5f3``` ou ```Ne5f3```. Esses lances tamb√©m representam o movimento de um cavalo para a casa ```f3```.

<p align="center">
  <img src='https://upload.wikimedia.org/wikipedia/commons/b/b6/SCD_algebraic_notation.svg' width='300'>
</p>

Um arquivo PGN completo tem o seguinte formato

```
[Event "F/S Return Match"]
[Site "Belgrade, Serbia JUG"]
[Date "1992.11.04"]
[Round "29"]
[White "Fischer, Robert J."]
[Black "Spassky, Boris V."]
[Result "1/2-1/2"]

1. e4 e5 2. Nf3 Nc6 3. Bb5 a6 {This opening is called the Ruy Lopez.}
4. Ba4 Nf6 5. O-O Be7 6. Re1 b5 7. Bb3 d6 8. c3 O-O 9. h3 Nb8 10. d4 Nbd7
11. c4 c6 12. cxb5 axb5 13. Nc3 Bb7 14. Bg5 b4 15. Nb1 h6 16. Bh4 c5 17. dxe5
Nxe4 18. Bxe7 Qxe7 19. exd6 Qf6 20. Nbd2 Nxd6 21. Nc4 Nxc4 22. Bxc4 Nb6
23. Ne5 Rae8 24. Bxf7+ Rxf7 25. Nxf7 Rxe1+ 26. Qxe1 Kxf7 27. Qe3 Qg5 28. Qxg5
hxg5 29. b3 Ke6 30. a3 Kd6 31. axb4 cxb4 32. Ra5 Nd5 33. f3 Bc8 34. Kf2 Bf5
35. Ra7 g6 36. Ra6+ Kc5 37. Ke1 Nf4 38. g3 Nxh3 39. Kd2 Kb5 40. Rd6 Kc5 41. Ra6
Nf2 42. g4 Bd3 43. Re6 1/2-1/2
```

Entretanto, ser√° usada apenas a parte que especifica os movimentos (*movetext*), sem a enumera√ß√£o das rodadas e sem coment√°rios, resultando numa nota√ß√£o similar √† seguinte:

```
e4 e5 Nf3 Nc6 Bb5 a6 Ba4 Nf6 O-O Be7 Re1 b5 Bb3 d6 c3 O-O h3 Nb8 d4 Nbd7
c4 c6 cxb5 axb5 Nc3 Bb7 Bg5 b4 Nb1 h6 Bh4 c5 dxe5 Nxe4 Bxe7 Qxe7 exd6 Qf6 Nbd2 Nxd6
Nc4 Nxc4 Bxc4 Nb6 Ne5 Rae8 Bxf7+ Rxf7 Nxf7 Rxe1+ Qxe1 Kxf7 Qe3 Qg5 Qxg5 hxg5
b3 Ke6 a3 Kd6 axb4 cxb4 Ra5 Nd5 f3 Bc8 Kf2 Bf5 Ra7 g6 Ra6+ Kc5 Ke1 Nf4 g3 Nxh3 
Kd2 Kb5 Rd6 Kc5 Ra6 Nf2 g4 Bd3 Re6 1/2-1/2
```

A partida pode ser recuperada usando um leitor de PGN:

<p align="center">
  <img src='/src/visualization/board.gif' width='300'>
</p>

### V√≠deo de apresenta√ß√£o

## Metodologia Proposta

### Base de dados

A base de dados a ser usada √© um conjunto de v√°rias partidas em torneios, dispon√≠vel no site [The Week in Chess](https://theweekinchess.com/zips). Ao todo s√£o mais de 1400 arqivos *zip*, cada um contendo partidas de torneios profissionais ao redor do mundo em cada semana. Todas as partidas est√£o no formato PGN.

### Abordagens de modelagem generativa

A ideia inicial √© usar as anota√ß√µes de lances (ex: ```Nf3```, ```e4```, ```Be7```) como palavras individuais em um texto para servir de tokens. Da√≠ um modelo autorregressivo tipo BERT seria treinado a partir dessa tokeniza√ß√£o. Uma vez que o modelo √© treinado, a gera√ß√£o de novas partidas ou complementos de partidas iniciadas pode ocorrer com a gera√ß√£o de um token por vez ou atrav√©s de *beam search*, onde os pr√≥ximos **k** lances s√£o escolhidos de modo a maximizar a probabilidade de acordo com o modelo. A desvantagem do beam search est√° no custo computacional elevado, crescendo exponencialmente com o tamanho de **k**.

### Artigos de refer√™ncia

1. [The Chess Transformer: Mastering Play using Generative Language Models](https://arxiv.org/abs/2008.04057) (2020). David Noever, Matt Ciolino, Josh Kalin.
2. [The Go Transformer: Natural Language Modeling for Game Play](https://arxiv.org/abs/2007.03500) (2020). Matthew Ciolino, David Noever, Josh Kalin.
3. [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (2017). Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin.
4. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) (2018). Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova.

### Ferramentas

1. [Python 3.7](https://www.python.org/downloads/release/python-370/) ou superior.
2. [Google Colab](https://colab.research.google.com/?utm_source=scs-index) para suporte √† execu√ß√£o de notebooks na nuvem com GPU.
3. Biblioteca [python-chess](https://python-chess.readthedocs.io/en/latest/) para processar as partidas no formato PGN.
4. Biblioteca [Hugginface Transformers](https://huggingface.co/docs/transformers/index) para carregar e treinar os modelos de linguagem.

### Resultados esperados

Espera-se que o modelo generativo crie partidas que sejam coerentes do ponto de vista do Xadrez profissional, pelo menos at√© a fase de abertura do jogo. Tamb√©m espera-se que o modelo seja capaz de continuar partidas com lances b√°sicos.

### Proposta de avalia√ß√£o

A avalia√ß√£o poder√° ser tanto objetiva quanto subjetiva. Na primeira, √© poss√≠vel usar *engines* de Xadrez, que s√£o programas de computador capazes de quantificar a qualidade de uma posi√ß√£o para ambos os jogadores. Os escores gerados pelas engines podem ser usados como m√©trica para a qualidade dos lances gerados pelo modelo. Existem v√°rias engines de Xadrez acess√≠veis, como as usadas no [Chess.com](https://chess.com) e no [Lichess](https://lichess.org). Na forma subjetiva, √© preciso pedir a avalia√ß√£o de algum jogador de Xadrez com experi√™ncia em torneios. Entre as duas formas de avalia√ß√£o, a forma objetiva √© a mais fact√≠vel, embora uma avalia√ß√£o subjetiva possa detectar tend√™ncias de jogo e estilo.

## Cronograma

| Tarefa  | 27 de Abril | 04 de Maio | 11 de Maio | 18 de Maio | 25 de Maio | 01 de Junho | 08 de Junho |
| --      | --          | --         | --         | --         | --         | --          | --          |
| Primeira Entrega  | üü¢ |  |  |  |  |  |  |
| Coleta e tratamento da base  | üü¢ | üü¢ |  |  |  |  |  |
| Primeiras baselines  |  |  | üü¢ |  |  |  |  |
| Treinamento do modelo final  |  |  |  | üü¢ | üü¢ |  |  |
| Avalia√ß√£o objetiva do modelo final  |  |  |  |  |  | üü¢ |  |
| * Avalia√ß√£o subjetiva do modelo final  |  |  |  |  |  | üü¢ |  |
| Escrita de relat√≥rio  |  |  |  |  |  |  | üü¢ |

## Refer√™ncias Bibliogr√°ficas

1. ["Standard: Portable Game Notation Specification and Implementation Guide"](https://archive.org/details/pgn-standard-1994-03-12). 12 March 1994.
2. ["Language Models are Few-Shot Learners"](https://arxiv.org/abs/2005.14165) (2020). Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei.
3. ["Evaluating Large Language Models Trained on Code"](https://arxiv.org/abs/2107.03374) (2021). Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, Wojciech Zaremba.
4. ["DeepSVG: A Hierarchical Generative Network for Vector Graphics Animation"](https://arxiv.org/abs/2007.11301) (2020). Alexandre Carlier, Martin Danelljan, Alexandre Alahi, Radu Timofte.
5. ["Generating Music Transition by Using a Transformer-Based Model"](https://www.mdpi.com/2079-9292/10/18/2276) (2021). Jia-Lien Hsu and Shuh-Jiun Chang.
6. [The Chess Transformer: Mastering Play using Generative Language Models](https://arxiv.org/abs/2008.04057) (2020). David Noever, Matt Ciolino, Josh Kalin.
7. [The Go Transformer: Natural Language Modeling for Game Play](https://arxiv.org/abs/2007.03500) (2020). Matthew Ciolino, David Noever, Josh Kalin.
8. [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (2017). Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin.
9. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) (2018). Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova.
